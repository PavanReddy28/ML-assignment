{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.150861</td>\n",
       "      <td>1.311693</td>\n",
       "      <td>1620.221779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>126.724861</td>\n",
       "      <td>1.302745</td>\n",
       "      <td>1609.334822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.173861</td>\n",
       "      <td>1.319031</td>\n",
       "      <td>1568.978435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.478861</td>\n",
       "      <td>1.270878</td>\n",
       "      <td>1695.055281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.262861</td>\n",
       "      <td>1.329637</td>\n",
       "      <td>1647.720235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157.498861</td>\n",
       "      <td>1.655794</td>\n",
       "      <td>5326.025889</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>152.404861</td>\n",
       "      <td>1.620345</td>\n",
       "      <td>5243.267754</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134.672861</td>\n",
       "      <td>1.541987</td>\n",
       "      <td>3766.763222</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>142.926861</td>\n",
       "      <td>1.426381</td>\n",
       "      <td>4118.327320</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133.920861</td>\n",
       "      <td>1.564621</td>\n",
       "      <td>3808.021317</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      att1  att2  att3        att4      att5         att6  class\n",
       "0      1.0   0.0   0.0  133.150861  1.311693  1620.221779      1\n",
       "1      1.0   0.0   0.0  126.724861  1.302745  1609.334822      1\n",
       "2      1.0   0.0   0.0  131.173861  1.319031  1568.978435      1\n",
       "3      1.0   0.0   0.0  129.478861  1.270878  1695.055281      1\n",
       "4      1.0   0.0   0.0  127.262861  1.329637  1647.720235      1\n",
       "...    ...   ...   ...         ...       ...          ...    ...\n",
       "1995   1.0   1.0   1.0  157.498861  1.655794  5326.025889     10\n",
       "1996   1.0   1.0   1.0  152.404861  1.620345  5243.267754     10\n",
       "1997   1.0   1.0   1.0  134.672861  1.541987  3766.763222     10\n",
       "1998   1.0   1.0   1.0  142.926861  1.426381  4118.327320     10\n",
       "1999   1.0   1.0   1.0  133.920861  1.564621  3808.021317     10\n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_NN.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    200\n",
       "9     200\n",
       "8     200\n",
       "7     200\n",
       "6     200\n",
       "5     200\n",
       "4     200\n",
       "3     200\n",
       "2     200\n",
       "1     200\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:,:-1]\n",
    "y = df.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data,split,randomize=True):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        data: numpy array of the dataset\n",
    "        split: percentage of the samples required for the training data\n",
    "        randomize: boolean representing whether you want to randomize the dataset\n",
    "    Returns:\n",
    "        train_X: numpy array containing training data\n",
    "        train_y: numpy array containing the training labels\n",
    "        test_X: numpy array containing testing data\n",
    "        test_y: numpy array containing the testing labels\n",
    "    \"\"\"\n",
    "    \n",
    "    split_index = int(split*len(data))\n",
    "    \n",
    "    # randomly shuffles rows of the dataset\n",
    "    if randomize == True:\n",
    "        np.random.shuffle(data)\n",
    "    \n",
    "    \n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    \n",
    "    train_X = X[:split_index]\n",
    "    train_y = y[:split_index]\n",
    "    \n",
    "    test_X = X[split_index:]\n",
    "    test_y = y[split_index:]\n",
    "    \n",
    "    return X,y,train_X,train_y,test_X,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,train_X,train_y,test_X,test_y = train_test_split(df.values,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 6)\n",
      "(2000,)\n",
      "(1400, 6)\n",
      "(1400,)\n",
      "(600, 6)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(train_X,X):\n",
    "    \n",
    "    concatenated = None\n",
    "    \n",
    "    for i in range(3):\n",
    "        \n",
    "        oh_feature = np.zeros((train_X.shape[0],np.unique(X[:,i]).shape[0]))\n",
    "        \n",
    "        \n",
    "        for index, feature in enumerate(train_X[:,i]):\n",
    "            oh_feature[index,int(feature - 1)] = 1\n",
    "        \n",
    "        if concatenated is not None:\n",
    "            concatenated = np.concatenate((concatenated,oh_feature), axis=1)\n",
    "        \n",
    "        else:\n",
    "            concatenated = oh_feature\n",
    "        \n",
    "    \n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_inputs = one_hot_encoder(train_X,X)\n",
    "oh_inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizer(X):\n",
    "    \n",
    "    X_continuous = X[:,3:6]\n",
    "    \n",
    "    X_mean = np.mean(X_continuous,axis=0)\n",
    "    X_std = np.std(X_continuous, axis=0)\n",
    "    \n",
    "    return (X_continuous - X_mean)/X_std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_inputs = standardizer(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = np.concatenate((oh_inputs,cont_inputs),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 19)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    \n",
    "    y_oh = np.zeros((10,y.shape[0]))\n",
    "    \n",
    "    for index,label in enumerate(y):\n",
    "        y_oh[int(label) - 1,index] = 1\n",
    "    \n",
    "    return y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layer_dimensions):\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for l in range(1,len(layer_dimensions)):\n",
    "        params['W' + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l-1])*0.01\n",
    "        \n",
    "        params['b' + str(l)] = np.random.randn(layer_dimensions[l], 1)*0.01\n",
    "    \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z):\n",
    "    # Z = (n_prev,m)\n",
    "    \n",
    "    #try max subtraction\n",
    "    \n",
    "    max_activations = np.max(Z,axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return np.exp(Z-max_activations)/np.sum(np.exp(Z-max_activations),axis=0,keepdims=True),Z\n",
    "#     return np.exp(Z)/np.sum(np.exp(Z),axis=0,keepdims=True),Z\n",
    "\n",
    "def relu_derivative(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_forward(A,W,b):\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    cache = (A,W,b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_activation(A_prev, W, b, activation_type):\n",
    "    \n",
    "    Z,linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation_type == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    elif activation_type == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_forward(X,parameters):\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        A_prev = A\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(\"multiplying weights of shape\")\n",
    "#         print(parameters[\"W\"+str(l)].shape)\n",
    "#         print(\"with activations of shape\")\n",
    "#         print(A_prev.shape)\n",
    "#         print(\"and adding bias of shape\")\n",
    "#         print(parameters[\"b\"+str(l)].shape)\n",
    "\n",
    "\n",
    "        Z = np.dot(parameters[\"W\"+str(l)],A_prev) + parameters[\"b\"+str(l)]\n",
    "    \n",
    "        linear_cache = (A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)])\n",
    "        \n",
    "        if l == L:\n",
    "            \n",
    "            A, activation_cache = softmax(Z)\n",
    "        \n",
    "        else:\n",
    "            A, activation_cache = relu(Z)\n",
    "    \n",
    "        cache = (linear_cache, activation_cache)\n",
    "        \n",
    "        caches.append(cache)\n",
    "    \n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_cost(yhats,y):\n",
    "    \n",
    "    m = y.shape[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    cost = -np.sum(np.log(np.maximum(yhats,1e-15))*y)/m\n",
    "    \n",
    "    \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_backward(dZ, cache):\n",
    "    \n",
    "    A_prev,W,b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.squeeze(np.sum(dZ, axis=1, keepdims=True))/m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_activation_backward(dA,cache,activation_type):\n",
    "    \n",
    "    lin_cache, act_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = lin_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_backward(yhats,y,caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    \n",
    "    m = yhats.shape[1]\n",
    "    \n",
    "    dZ = yhats - y\n",
    "    \n",
    "    cache = caches[L-1]\n",
    "\n",
    "    linear_cache,activation_cache = cache\n",
    "    A_prev, W, b = linear_cache\n",
    "    \n",
    "    \n",
    "    \n",
    "    grads[\"dW\" + str(L)] = np.dot(dZ,A_prev.T)/m\n",
    "    grads[\"db\" + str(L)] = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    grads[\"dA\" + str(L-1)] = np.dot(W.T,dZ)\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        linear_cache, activation_cache = caches[l]\n",
    "        \n",
    "        dZ = relu_derivative(grads[\"dA\"+str(l+1)],activation_cache)\n",
    "        \n",
    "        A_prev,W,b = linear_cache\n",
    "        \n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        dW = np.dot(dZ,A_prev.T)\n",
    "        \n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        \n",
    "        grads[\"dA\"+str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(parameters, grads, lr):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        \n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - lr*grads[\"dW\"+str(l+1)]\n",
    "        \n",
    "        \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - lr*grads[\"db\"+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_generator(X,y,mini_batch_size):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_y = y[:, permutation]\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_y = shuffled_y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_y = shuffled_y[:,num_complete_minibatches * mini_batch_size:]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,y, layer_dims, learning_rate, epochs,mini_batch_size):\n",
    "    \n",
    "    parameters = initialize(layer_dims)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        minibatches = minibatch_generator(X,y,mini_batch_size)\n",
    "        \n",
    "        predictions = None\n",
    "        labels = None\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            \n",
    "            mini_batch_X,mini_batch_y = minibatch\n",
    "            \n",
    "        \n",
    "            yhats, caches = NN_forward(mini_batch_X,parameters)\n",
    "        \n",
    "#             predictions_categorical = np.argmax(yhats,axis=0)\n",
    "\n",
    "#             true_categorical = np.argmax(mini_batch_y,axis=0)\n",
    "    \n",
    "            if predictions is None:\n",
    "                predictions = yhats\n",
    "            \n",
    "            else:\n",
    "                predictions = np.concatenate((predictions,yhats),axis=1)\n",
    "                \n",
    "            if labels is None:\n",
    "                labels = mini_batch_y\n",
    "            else:\n",
    "                labels = np.concatenate((labels,mini_batch_y),axis=1)\n",
    "        \n",
    "            cost = NN_cost(yhats,mini_batch_y)\n",
    "\n",
    "            grads = NN_backward(yhats,mini_batch_y,caches)\n",
    "        \n",
    "            parameters = update(parameters, grads, learning_rate)\n",
    "        \n",
    "            costs.append(cost)\n",
    "            \n",
    "            \n",
    "        predictions_categorical = np.argmax(predictions,axis=0)\n",
    "\n",
    "        true_categorical = np.argmax(labels,axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"epoch: \"+str(epoch))\n",
    "            print(\"Training accuracy\")\n",
    "            print(np.sum((predictions_categorical == true_categorical))/y.shape[1])\n",
    "            print(\"Cost: \"+str(cost))\n",
    "    \n",
    "    return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [19,18,8,10]\n",
    "y_oh = one_hot(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Training accuracy\n",
      "0.10071428571428571\n",
      "Cost: 2.302702282319983\n",
      "epoch: 1000\n",
      "Training accuracy\n",
      "0.10285714285714286\n",
      "Cost: 2.303654350879502\n",
      "epoch: 2000\n",
      "Training accuracy\n",
      "0.10285714285714286\n",
      "Cost: 2.299369954166128\n",
      "epoch: 3000\n",
      "Training accuracy\n",
      "0.13428571428571429\n",
      "Cost: 2.305711800976897\n",
      "epoch: 4000\n",
      "Training accuracy\n",
      "0.21142857142857144\n",
      "Cost: 2.0064025483841057\n",
      "epoch: 5000\n",
      "Training accuracy\n",
      "0.3485714285714286\n",
      "Cost: 1.3052456223935627\n",
      "epoch: 6000\n",
      "Training accuracy\n",
      "0.5728571428571428\n",
      "Cost: 1.0759631536669643\n",
      "epoch: 7000\n",
      "Training accuracy\n",
      "0.6007142857142858\n",
      "Cost: 0.8788328084526384\n",
      "epoch: 8000\n",
      "Training accuracy\n",
      "0.6378571428571429\n",
      "Cost: 0.3687186054477679\n",
      "epoch: 9000\n",
      "Training accuracy\n",
      "0.6521428571428571\n",
      "Cost: 1.3425013238482753\n",
      "epoch: 10000\n",
      "Training accuracy\n",
      "0.6721428571428572\n",
      "Cost: 0.9163168120242828\n",
      "epoch: 11000\n",
      "Training accuracy\n",
      "0.7128571428571429\n",
      "Cost: 0.37092884256407943\n",
      "epoch: 12000\n",
      "Training accuracy\n",
      "0.7285714285714285\n",
      "Cost: 1.0189007116507367\n",
      "epoch: 13000\n",
      "Training accuracy\n",
      "0.7421428571428571\n",
      "Cost: 0.7267950929444228\n",
      "epoch: 14000\n",
      "Training accuracy\n",
      "0.7471428571428571\n",
      "Cost: 0.3658753494338781\n",
      "epoch: 15000\n",
      "Training accuracy\n",
      "0.7514285714285714\n",
      "Cost: 0.6106694163962567\n",
      "epoch: 16000\n",
      "Training accuracy\n",
      "0.7557142857142857\n",
      "Cost: 1.419204935028742\n",
      "epoch: 17000\n",
      "Training accuracy\n",
      "0.7564285714285715\n",
      "Cost: 0.3129520314862323\n",
      "epoch: 18000\n",
      "Training accuracy\n",
      "0.7557142857142857\n",
      "Cost: 0.6034968240308085\n",
      "epoch: 19000\n",
      "Training accuracy\n",
      "0.7614285714285715\n",
      "Cost: 0.5720710628599037\n",
      "epoch: 20000\n",
      "Training accuracy\n",
      "0.7578571428571429\n",
      "Cost: 0.6232605337233655\n",
      "epoch: 21000\n",
      "Training accuracy\n",
      "0.7564285714285715\n",
      "Cost: 0.273539563222764\n",
      "epoch: 22000\n",
      "Training accuracy\n",
      "0.76\n",
      "Cost: 0.5563254449608518\n",
      "epoch: 23000\n",
      "Training accuracy\n",
      "0.7578571428571429\n",
      "Cost: 0.6045507669196931\n",
      "epoch: 24000\n",
      "Training accuracy\n",
      "0.7535714285714286\n",
      "Cost: 0.3522017482235998\n",
      "epoch: 25000\n",
      "Training accuracy\n",
      "0.7592857142857142\n",
      "Cost: 0.4271656493636957\n",
      "epoch: 26000\n",
      "Training accuracy\n",
      "0.7578571428571429\n",
      "Cost: 0.731221985156336\n",
      "epoch: 27000\n",
      "Training accuracy\n",
      "0.7578571428571429\n",
      "Cost: 0.31960121662707686\n",
      "epoch: 28000\n",
      "Training accuracy\n",
      "0.7571428571428571\n",
      "Cost: 1.0424163105121158\n",
      "epoch: 29000\n",
      "Training accuracy\n",
      "0.7642857142857142\n",
      "Cost: 0.5887452819452078\n",
      "epoch: 30000\n",
      "Training accuracy\n",
      "0.7685714285714286\n",
      "Cost: 0.3589626814114808\n",
      "epoch: 31000\n",
      "Training accuracy\n",
      "0.7635714285714286\n",
      "Cost: 0.29394526467930726\n",
      "epoch: 32000\n",
      "Training accuracy\n",
      "0.7642857142857142\n",
      "Cost: 0.3938453670370875\n",
      "epoch: 33000\n",
      "Training accuracy\n",
      "0.7614285714285715\n",
      "Cost: 0.6095411317355521\n",
      "epoch: 34000\n",
      "Training accuracy\n",
      "0.7742857142857142\n",
      "Cost: 0.4741986861388088\n",
      "epoch: 35000\n",
      "Training accuracy\n",
      "0.765\n",
      "Cost: 0.3617643055214811\n",
      "epoch: 36000\n",
      "Training accuracy\n",
      "0.765\n",
      "Cost: 0.953252557696254\n",
      "epoch: 37000\n",
      "Training accuracy\n",
      "0.7671428571428571\n",
      "Cost: 0.9814394747717218\n",
      "epoch: 38000\n",
      "Training accuracy\n",
      "0.7707142857142857\n",
      "Cost: 0.2013157138966572\n",
      "epoch: 39000\n",
      "Training accuracy\n",
      "0.7707142857142857\n",
      "Cost: 0.6425429349355155\n",
      "epoch: 40000\n",
      "Training accuracy\n",
      "0.77\n",
      "Cost: 0.8881432949159538\n",
      "epoch: 41000\n",
      "Training accuracy\n",
      "0.7707142857142857\n",
      "Cost: 0.49710784151229936\n",
      "epoch: 42000\n",
      "Training accuracy\n",
      "0.775\n",
      "Cost: 0.4943121966411722\n",
      "epoch: 43000\n",
      "Training accuracy\n",
      "0.7721428571428571\n",
      "Cost: 0.4106683486996697\n",
      "epoch: 44000\n",
      "Training accuracy\n",
      "0.7728571428571429\n",
      "Cost: 0.12200382448878855\n",
      "epoch: 45000\n",
      "Training accuracy\n",
      "0.7671428571428571\n",
      "Cost: 0.5017003889954639\n",
      "epoch: 46000\n",
      "Training accuracy\n",
      "0.7735714285714286\n",
      "Cost: 0.3423467282264814\n",
      "epoch: 47000\n",
      "Training accuracy\n",
      "0.7692857142857142\n",
      "Cost: 0.21428388880256635\n",
      "epoch: 48000\n",
      "Training accuracy\n",
      "0.77\n",
      "Cost: 1.4099414023144399\n",
      "epoch: 49000\n",
      "Training accuracy\n",
      "0.7764285714285715\n",
      "Cost: 0.5308602468260224\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_scaled.T,y_oh,layer_dims,0.0001,50000,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_inputs_test = one_hot_encoder(test_X,X)\n",
    "oh_inputs_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_inputs_test = standardizer(test_X)\n",
    "cont_inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 19)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled = np.concatenate((oh_inputs_test,cont_inputs_test),axis=1)\n",
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_X,parameters):\n",
    "    \n",
    "    preds, caches = NN_forward(test_X,parameters)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459\n"
     ]
    }
   ],
   "source": [
    "preds = predict(X_test_scaled.T,parameters)\n",
    "\n",
    "y_test_oh = one_hot(test_y)\n",
    "\n",
    "predictions_categorical = np.argmax(preds,axis=0)\n",
    "\n",
    "true_categorical = np.argmax(y_test_oh,axis=0)\n",
    "\n",
    "print(np.sum((predictions_categorical == true_categorical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(train_X,X):\n",
    "    \n",
    "    concatenated = None\n",
    "    \n",
    "    for i in range(3):\n",
    "        \n",
    "        oh_feature = np.zeros((train_X.shape[0],np.unique(X[:,i]).shape[0]))\n",
    "        \n",
    "        \n",
    "        for index, feature in enumerate(train_X[:,i]):\n",
    "            oh_feature[index,int(feature - 1)] = 1\n",
    "        \n",
    "        if concatenated is not None:\n",
    "            concatenated = np.concatenate((concatenated,oh_feature), axis=1)\n",
    "        \n",
    "        else:\n",
    "            concatenated = oh_feature\n",
    "        \n",
    "    \n",
    "    return concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_inputs = one_hot_encoder(train_X,X)\n",
    "oh_inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_inputs_test = one_hot_encoder(test_X,X)\n",
    "oh_inputs_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_continuous = train_X[:,3:6]\n",
    "\n",
    "\n",
    "X_non_scaled = np.concatenate((oh_inputs,train_X_continuous),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 19)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_non_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizer(X):\n",
    "    \n",
    "    X_continuous = X[:,3:6]\n",
    "    \n",
    "    X_mean = np.mean(X_continuous,axis=0)\n",
    "    X_std = np.std(X_continuous, axis=0)\n",
    "    \n",
    "    return (X_continuous - X_mean)/X_std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_inputs = standardizer(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_inputs_test = standardizer(test_X)\n",
    "cont_inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 19)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = np.concatenate((oh_inputs,cont_inputs),axis=1)\n",
    "X_test_scaled = np.concatenate((oh_inputs_test,cont_inputs_test),axis=1)\n",
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    \n",
    "    y_oh = np.zeros((10,y.shape[0]))\n",
    "    \n",
    "    for index,label in enumerate(y):\n",
    "        y_oh[int(label) - 1,index] = 1\n",
    "    \n",
    "    return y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oh = one_hot(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN():\n",
    "    \n",
    "    def __init__(self,layer_dims):\n",
    "        \n",
    "        self.layer_dims = layer_dims\n",
    "        \n",
    "    def initialize_params(self):\n",
    "\n",
    "        self.parameters = {}\n",
    "\n",
    "        for l in range(1,len(self.layer_dims)):\n",
    "            self.parameters['W' + str(l)] = np.random.randn(self.layer_dims[l], self.layer_dims[l-1])*0.01\n",
    "\n",
    "            self.parameters['b' + str(l)] = np.random.randn(self.layer_dims[l], 1)*0.01\n",
    "\n",
    "        \n",
    "    \n",
    "    def relu(self,Z):\n",
    "\n",
    "        A = np.maximum(0,Z)\n",
    "\n",
    "        cache = Z\n",
    "        \n",
    "        return A, cache\n",
    "\n",
    "    def softmax(self,Z):\n",
    "\n",
    "        max_activations = np.max(Z,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        return np.exp(Z-max_activations)/np.sum(np.exp(Z-max_activations),axis=0,keepdims=True),Z\n",
    "    \n",
    "\n",
    "    def relu_derivative(self,dA, cache):\n",
    "\n",
    "        Z = cache\n",
    "        dZ = np.array(dA, copy=True)\n",
    "\n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "    def NN_forward(self,X,parameters):\n",
    "\n",
    "        caches = []\n",
    "        A = X\n",
    "        L = len(parameters) // 2\n",
    "\n",
    "        for l in range(1,L+1):\n",
    "            A_prev = A\n",
    "\n",
    "\n",
    "            Z = np.dot(parameters[\"W\"+str(l)],A_prev) + parameters[\"b\"+str(l)]\n",
    "\n",
    "            linear_cache = (A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)])\n",
    "\n",
    "            if l == L:\n",
    "\n",
    "                A, activation_cache = self.softmax(Z)\n",
    "\n",
    "            else:\n",
    "                A, activation_cache = self.relu(Z)\n",
    "\n",
    "            cache = (linear_cache, activation_cache)\n",
    "\n",
    "            caches.append(cache)\n",
    "\n",
    "        return A, caches\n",
    "\n",
    "    \n",
    "    def NN_cost(self,predictions,true):\n",
    "\n",
    "        m = true.shape[1]\n",
    "\n",
    "        cost = -np.sum(np.log(np.maximum(predictions,1e-15))*true)/m\n",
    "\n",
    "        return cost\n",
    "\n",
    "    \n",
    "    def NN_backward(self,predictions,true,caches):\n",
    "\n",
    "        grads = {}\n",
    "        L = len(caches)\n",
    "\n",
    "        m = predictions.shape[1]\n",
    "\n",
    "        dZ = predictions - true\n",
    "\n",
    "        cache = caches[L-1]\n",
    "\n",
    "        linear_cache,activation_cache = cache\n",
    "        A_prev, W, b = linear_cache\n",
    "\n",
    "\n",
    "\n",
    "        grads[\"dW\" + str(L)] = np.dot(dZ,A_prev.T)/m\n",
    "        grads[\"db\" + str(L)] = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "        grads[\"dA\" + str(L-1)] = np.dot(W.T,dZ)\n",
    "\n",
    "        for l in reversed(range(L-1)):\n",
    "            linear_cache, activation_cache = caches[l]\n",
    "\n",
    "            dZ = self.relu_derivative(grads[\"dA\"+str(l+1)],activation_cache)\n",
    "\n",
    "            A_prev,W,b = linear_cache\n",
    "\n",
    "            m = A_prev.shape[1]\n",
    "\n",
    "            dW = np.dot(dZ,A_prev.T)\n",
    "\n",
    "            db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "            grads[\"dA\"+str(l)] = dA_prev\n",
    "            grads[\"dW\" + str(l + 1)] = dW\n",
    "            grads[\"db\" + str(l + 1)] = db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    \n",
    "    def update(self,parameters, grads, lr):\n",
    "\n",
    "        L = len(parameters) // 2\n",
    "\n",
    "        for l in range(L):\n",
    "\n",
    "\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - lr*grads[\"dW\"+str(l+1)]\n",
    "\n",
    "\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - lr*grads[\"db\"+str(l+1)]\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    \n",
    "    def minibatch_generator(self,X,y,mini_batch_size):\n",
    "\n",
    "        m = X.shape[1]\n",
    "        mini_batches = []\n",
    "\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_y = y[:, permutation]\n",
    "\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "            mini_batch_y = shuffled_y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "            mini_batch = (mini_batch_X, mini_batch_y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        if m % mini_batch_size != 0:\n",
    "\n",
    "            mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n",
    "            mini_batch_y = shuffled_y[:,num_complete_minibatches * mini_batch_size:]\n",
    "\n",
    "            mini_batch = (mini_batch_X, mini_batch_y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        return mini_batches\n",
    "    \n",
    "\n",
    "    def fit(self,X,y, learning_rate, epochs,mini_batch_size):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "\n",
    "        self.initialize_params()\n",
    "\n",
    "        self.costs = []\n",
    "        self.accuracies = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            minibatches = self.minibatch_generator(X,y,self.mini_batch_size)\n",
    "\n",
    "            predictions = None\n",
    "            labels = None\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                mini_batch_X,mini_batch_y = minibatch\n",
    "\n",
    "\n",
    "                batch_predictions, caches = self.NN_forward(mini_batch_X,self.parameters)\n",
    "\n",
    "\n",
    "                if predictions is None:\n",
    "                    predictions = batch_predictions\n",
    "\n",
    "                else:\n",
    "                    predictions = np.concatenate((predictions,batch_predictions),axis=1)\n",
    "\n",
    "                if labels is None:\n",
    "                    labels = mini_batch_y\n",
    "                else:\n",
    "                    labels = np.concatenate((labels,mini_batch_y),axis=1)\n",
    "\n",
    "                cost = self.NN_cost(batch_predictions,mini_batch_y)\n",
    "\n",
    "                grads = self.NN_backward(batch_predictions,mini_batch_y,caches)\n",
    "\n",
    "                self.parameters = self.update(self.parameters, grads, self.learning_rate)\n",
    "\n",
    "            self.costs.append(cost)\n",
    "\n",
    "\n",
    "            predictions_categorical = np.argmax(predictions,axis=0)\n",
    "\n",
    "            true_categorical = np.argmax(labels,axis=0)\n",
    "            \n",
    "            accuracy = np.sum((predictions_categorical == true_categorical))/y.shape[1]\n",
    "            \n",
    "            self.accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "            if (epoch+1) % 1000 == 0:\n",
    "                print(\"epoch: {}\".format(epoch))\n",
    "                \n",
    "                print(\"Training accuracy {:.2f}\".format(accuracy*100))\n",
    "                \n",
    "                print(\"Cost: {}\".format(cost))\n",
    "                print()\n",
    "\n",
    "        return {\n",
    "            \"costs\":self.costs,\n",
    "            \"accuracis\": self.accuracies\n",
    "        }\n",
    "\n",
    "    def predict(self,X):\n",
    "\n",
    "        predictions, caches = self.NN_forward(X,self.parameters)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [19,18,15,10]\n",
    "ann = ANN(layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 999\n",
      "Training accuracy 10.71\n",
      "Cost: 2.301577711916843\n",
      "\n",
      "epoch: 1999\n",
      "Training accuracy 10.71\n",
      "Cost: 2.2997374058250832\n",
      "\n",
      "epoch: 2999\n",
      "Training accuracy 14.36\n",
      "Cost: 2.298991838943966\n",
      "\n",
      "epoch: 3999\n",
      "Training accuracy 10.93\n",
      "Cost: 2.043692257136743\n",
      "\n",
      "epoch: 4999\n",
      "Training accuracy 20.57\n",
      "Cost: 1.9110507221868256\n",
      "\n",
      "epoch: 5999\n",
      "Training accuracy 20.64\n",
      "Cost: 2.0197547616375235\n",
      "\n",
      "epoch: 6999\n",
      "Training accuracy 21.36\n",
      "Cost: 1.911304716445968\n",
      "\n",
      "epoch: 7999\n",
      "Training accuracy 21.86\n",
      "Cost: 1.7658689063237125\n",
      "\n",
      "epoch: 8999\n",
      "Training accuracy 22.50\n",
      "Cost: 1.9143711362086726\n",
      "\n",
      "epoch: 9999\n",
      "Training accuracy 22.93\n",
      "Cost: 1.6943289650569366\n",
      "\n",
      "epoch: 10999\n",
      "Training accuracy 24.29\n",
      "Cost: 1.759697746743606\n",
      "\n",
      "epoch: 11999\n",
      "Training accuracy 42.29\n",
      "Cost: 1.5465508737153928\n",
      "\n",
      "epoch: 12999\n",
      "Training accuracy 54.71\n",
      "Cost: 1.3533350075128092\n",
      "\n",
      "epoch: 13999\n",
      "Training accuracy 58.14\n",
      "Cost: 1.073281507439884\n",
      "\n",
      "epoch: 14999\n",
      "Training accuracy 65.86\n",
      "Cost: 0.8428154996022847\n",
      "\n",
      "epoch: 15999\n",
      "Training accuracy 71.29\n",
      "Cost: 0.6048765142019719\n",
      "\n",
      "epoch: 16999\n",
      "Training accuracy 72.43\n",
      "Cost: 0.7286164732707824\n",
      "\n",
      "epoch: 17999\n",
      "Training accuracy 73.14\n",
      "Cost: 0.9586922597021579\n",
      "\n",
      "epoch: 18999\n",
      "Training accuracy 73.93\n",
      "Cost: 0.5611052220646664\n",
      "\n",
      "epoch: 19999\n",
      "Training accuracy 74.79\n",
      "Cost: 0.5137577932261622\n",
      "\n",
      "epoch: 20999\n",
      "Training accuracy 74.79\n",
      "Cost: 0.5155620215953236\n",
      "\n",
      "epoch: 21999\n",
      "Training accuracy 74.64\n",
      "Cost: 0.47575137061879386\n",
      "\n",
      "epoch: 22999\n",
      "Training accuracy 74.71\n",
      "Cost: 0.7243346644825337\n",
      "\n",
      "epoch: 23999\n",
      "Training accuracy 75.14\n",
      "Cost: 0.6048898778597394\n",
      "\n",
      "epoch: 24999\n",
      "Training accuracy 74.50\n",
      "Cost: 0.5877055694910298\n",
      "\n",
      "epoch: 25999\n",
      "Training accuracy 74.43\n",
      "Cost: 0.6328997279905612\n",
      "\n",
      "epoch: 26999\n",
      "Training accuracy 74.93\n",
      "Cost: 0.6123129566054504\n",
      "\n",
      "epoch: 27999\n",
      "Training accuracy 75.14\n",
      "Cost: 0.4460906734964792\n",
      "\n",
      "epoch: 28999\n",
      "Training accuracy 75.29\n",
      "Cost: 0.7407266541883717\n",
      "\n",
      "epoch: 29999\n",
      "Training accuracy 75.57\n",
      "Cost: 0.5746263170222158\n",
      "\n",
      "epoch: 30999\n",
      "Training accuracy 75.79\n",
      "Cost: 0.6949428285380573\n",
      "\n",
      "epoch: 31999\n",
      "Training accuracy 75.64\n",
      "Cost: 0.5490129473514284\n",
      "\n",
      "epoch: 32999\n",
      "Training accuracy 75.93\n",
      "Cost: 0.6132432221160016\n",
      "\n",
      "epoch: 33999\n",
      "Training accuracy 75.86\n",
      "Cost: 0.46045187059165027\n",
      "\n",
      "epoch: 34999\n",
      "Training accuracy 75.71\n",
      "Cost: 0.5582179607334421\n",
      "\n",
      "epoch: 35999\n",
      "Training accuracy 75.64\n",
      "Cost: 0.5199470100850766\n",
      "\n",
      "epoch: 36999\n",
      "Training accuracy 75.57\n",
      "Cost: 0.6246356434760754\n",
      "\n",
      "epoch: 37999\n",
      "Training accuracy 75.57\n",
      "Cost: 0.7247681128569796\n",
      "\n",
      "epoch: 38999\n",
      "Training accuracy 75.50\n",
      "Cost: 0.41199329061726697\n",
      "\n",
      "epoch: 39999\n",
      "Training accuracy 75.79\n",
      "Cost: 0.4965447103014597\n",
      "\n",
      "epoch: 40999\n",
      "Training accuracy 75.29\n",
      "Cost: 0.5878375579173493\n",
      "\n",
      "epoch: 41999\n",
      "Training accuracy 75.64\n",
      "Cost: 0.6676052671031705\n",
      "\n",
      "epoch: 42999\n",
      "Training accuracy 75.50\n",
      "Cost: 0.49758646321415917\n",
      "\n",
      "epoch: 43999\n",
      "Training accuracy 75.64\n",
      "Cost: 0.40029257818859076\n",
      "\n",
      "epoch: 44999\n",
      "Training accuracy 75.57\n",
      "Cost: 0.48363265389064897\n",
      "\n",
      "epoch: 45999\n",
      "Training accuracy 75.57\n",
      "Cost: 0.4737906953132403\n",
      "\n",
      "epoch: 46999\n",
      "Training accuracy 75.50\n",
      "Cost: 0.45550516981269046\n",
      "\n",
      "epoch: 47999\n",
      "Training accuracy 75.93\n",
      "Cost: 0.4521107870117433\n",
      "\n",
      "epoch: 48999\n",
      "Training accuracy 76.00\n",
      "Cost: 0.5631738494080631\n",
      "\n",
      "epoch: 49999\n",
      "Training accuracy 76.21\n",
      "Cost: 0.5528926891263516\n",
      "\n",
      "epoch: 50999\n",
      "Training accuracy 76.00\n",
      "Cost: 0.5391195391528109\n",
      "\n",
      "epoch: 51999\n",
      "Training accuracy 76.14\n",
      "Cost: 0.5591457174335457\n",
      "\n",
      "epoch: 52999\n",
      "Training accuracy 76.21\n",
      "Cost: 0.4245547900182454\n",
      "\n",
      "epoch: 53999\n",
      "Training accuracy 76.21\n",
      "Cost: 0.4563158291986917\n",
      "\n",
      "epoch: 54999\n",
      "Training accuracy 76.29\n",
      "Cost: 0.4766399517583206\n",
      "\n",
      "epoch: 55999\n",
      "Training accuracy 76.07\n",
      "Cost: 0.571790058142532\n",
      "\n",
      "epoch: 56999\n",
      "Training accuracy 76.29\n",
      "Cost: 0.5734091888108502\n",
      "\n",
      "epoch: 57999\n",
      "Training accuracy 76.14\n",
      "Cost: 0.41196429290814623\n",
      "\n",
      "epoch: 58999\n",
      "Training accuracy 76.29\n",
      "Cost: 0.5777508869394461\n",
      "\n",
      "epoch: 59999\n",
      "Training accuracy 76.21\n",
      "Cost: 0.48523341017469157\n",
      "\n",
      "epoch: 60999\n",
      "Training accuracy 75.71\n",
      "Cost: 0.49102008723165774\n",
      "\n",
      "epoch: 61999\n",
      "Training accuracy 76.43\n",
      "Cost: 0.6592691922112154\n",
      "\n",
      "epoch: 62999\n",
      "Training accuracy 76.43\n",
      "Cost: 0.6591364222552902\n",
      "\n",
      "epoch: 63999\n",
      "Training accuracy 76.50\n",
      "Cost: 0.6049981189821679\n",
      "\n",
      "epoch: 64999\n",
      "Training accuracy 76.36\n",
      "Cost: 0.33695766836607743\n",
      "\n",
      "epoch: 65999\n",
      "Training accuracy 76.57\n",
      "Cost: 0.5677124650802214\n",
      "\n",
      "epoch: 66999\n",
      "Training accuracy 76.64\n",
      "Cost: 0.477280970173284\n",
      "\n",
      "epoch: 67999\n",
      "Training accuracy 76.71\n",
      "Cost: 0.6174909778155782\n",
      "\n",
      "epoch: 68999\n",
      "Training accuracy 76.50\n",
      "Cost: 0.30005374538021407\n",
      "\n",
      "epoch: 69999\n",
      "Training accuracy 76.64\n",
      "Cost: 0.5727618567503538\n",
      "\n",
      "epoch: 70999\n",
      "Training accuracy 76.64\n",
      "Cost: 0.4378794198421535\n",
      "\n",
      "epoch: 71999\n",
      "Training accuracy 76.71\n",
      "Cost: 0.6968116703071939\n",
      "\n",
      "epoch: 72999\n",
      "Training accuracy 76.36\n",
      "Cost: 0.4625602392093337\n",
      "\n",
      "epoch: 73999\n",
      "Training accuracy 76.50\n",
      "Cost: 0.7679983395212504\n",
      "\n",
      "epoch: 74999\n",
      "Training accuracy 77.00\n",
      "Cost: 0.3915259830787735\n",
      "\n",
      "epoch: 75999\n",
      "Training accuracy 77.29\n",
      "Cost: 0.3835322722231312\n",
      "\n",
      "epoch: 76999\n",
      "Training accuracy 77.21\n",
      "Cost: 0.5504042396880903\n",
      "\n",
      "epoch: 77999\n",
      "Training accuracy 77.29\n",
      "Cost: 0.5571834997342126\n",
      "\n",
      "epoch: 78999\n",
      "Training accuracy 77.64\n",
      "Cost: 0.5142188940378423\n",
      "\n",
      "epoch: 79999\n",
      "Training accuracy 76.86\n",
      "Cost: 0.46947961085858003\n",
      "\n",
      "epoch: 80999\n",
      "Training accuracy 77.21\n",
      "Cost: 0.4202993000573359\n",
      "\n",
      "epoch: 81999\n",
      "Training accuracy 76.93\n",
      "Cost: 0.2879098350638104\n",
      "\n",
      "epoch: 82999\n",
      "Training accuracy 77.21\n",
      "Cost: 0.4491250077265349\n",
      "\n",
      "epoch: 83999\n",
      "Training accuracy 77.00\n",
      "Cost: 0.4564242548379013\n",
      "\n",
      "epoch: 84999\n",
      "Training accuracy 77.36\n",
      "Cost: 0.40572173119782534\n",
      "\n",
      "epoch: 85999\n",
      "Training accuracy 77.36\n",
      "Cost: 0.5157683075309124\n",
      "\n",
      "epoch: 86999\n",
      "Training accuracy 77.29\n",
      "Cost: 0.4756026959398068\n",
      "\n",
      "epoch: 87999\n",
      "Training accuracy 77.71\n",
      "Cost: 0.46878293398870163\n",
      "\n",
      "epoch: 88999\n",
      "Training accuracy 77.50\n",
      "Cost: 0.5340401838361932\n",
      "\n",
      "epoch: 89999\n",
      "Training accuracy 77.57\n",
      "Cost: 0.6379134419923965\n",
      "\n",
      "epoch: 90999\n",
      "Training accuracy 77.71\n",
      "Cost: 0.4820094165623606\n",
      "\n",
      "epoch: 91999\n",
      "Training accuracy 77.93\n",
      "Cost: 0.6016003032731675\n",
      "\n",
      "epoch: 92999\n",
      "Training accuracy 77.86\n",
      "Cost: 0.5508455423237557\n",
      "\n",
      "epoch: 93999\n",
      "Training accuracy 77.64\n",
      "Cost: 0.5153206989240539\n",
      "\n",
      "epoch: 94999\n",
      "Training accuracy 77.71\n",
      "Cost: 0.5436488687168894\n",
      "\n",
      "epoch: 95999\n",
      "Training accuracy 78.00\n",
      "Cost: 0.389096191457263\n",
      "\n",
      "epoch: 96999\n",
      "Training accuracy 78.07\n",
      "Cost: 0.5789710839714614\n",
      "\n",
      "epoch: 97999\n",
      "Training accuracy 78.00\n",
      "Cost: 0.5528584520753225\n",
      "\n",
      "epoch: 98999\n",
      "Training accuracy 77.93\n",
      "Cost: 0.33505862131855596\n",
      "\n",
      "epoch: 99999\n",
      "Training accuracy 78.00\n",
      "Cost: 0.4225207137901781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = ann.fit(X_scaled.T,y_oh,0.00006,100000,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x217c126eca0>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[\"accuracis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x217c1595bb0>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAApl0lEQVR4nO3dd3wUdfoH8M+TkBAIJEAIIdSA9N4MoKL0quJZ7vCOn4qnnO3Orih6YrtD7uycYDnlPDuinEpRuiDSa4AAASJEShIISUgh7fv7Y2c3u5vZ7G6yu5Od/bxfL17szszOPN9N8sy3zYwopUBEROYTZnQARETkH0zwREQmxQRPRGRSTPBERCbFBE9EZFL1jDpw8+bNVVJSklGHJyIKStu3b89WSsV7sq1hCT4pKQnbtm0z6vBEREFJRH7xdFt20RARmRQTPBGRSTHBExGZFBM8EZFJMcETEZkUEzwRkUkxwRMRmZRh8+D9IbewFKfzipF6Og/5xWWYOqS90SERERkm6BL8kj2ncO8nOzza9qnFKbh7+CV4fHy3arfLLSpFbIMIX4RHRFRnBF0XzaajZ73aft7aI6juoSbb0s+h77M/4Id9p93uKz27AMWl5V4dn4jIKEFXg3/+ul54bnJPXCyrQF5xKXYeP4/FO3/FoTP5OJJVoPuZ03nFSIxtoLtu14nzAIBNR89hbM+WLo9bWl6B4f9ci9HdE/DerYNqXQ4iIn8LugQPACKCqIhwREWEY1zPlhjnIjEnzVgCAJj63maseni42/2ezi3G8pRTuO3yDlXWlVdYWgE/Hs6qeeBERAEUdF00NWFfs9+afg5JM5bg6cUpVba788NtmPXtfpw4VxjI8IiI/CIkEry9h7/YDQD476aqN2TLLSoFAFRU9yByPqOciIJEUHbR1MSjC3dj4fYMxEVH6q4XqXytl9/t1xMRBYOQqcEv3J4BoLKW7qy8QlVbcxdYMnxJeQU+2XwcFRWsyhNR3WbqBD+gXZMqy8pcJOYFG9ORkVMEQL8Xxr4G/+TXe/Hd3lM+iJCIyH9MneAviW/kdhu9SntOYYnbz+UXl+LHQ1mcF09EdZapE/w1fVu53UavW+bJr/YCsFwENfTvq3Asu+r8+vnrjuCW97fg4YW7ax8oEZEfmDrBu+sl/3jzL/j7stQqywtKyjBneSpunP8zTuUW4601aXAeYz1xztKds2QPu2qIqG4ydYJ3NxA68+uqc+EBS/J+a+0R2/uF2zM4O5KIgo65E3x189mJiEzO1Am+T5smPtsXp8ETUbAxdYJv5uKippoQXulEREHG1Anelyn5TF6xD/dGROR/5k7wPszwl81e7budEREFgKkTPBFRKDN1gg9Uv/mGw9m4af5G2z3jiYjqgpC5m6Q/PfD5TmRfKMG5ghLEN65vdDhERABMXoMnIgplTPA+kH3B/c3JiIgCjQnehzhVnojqEiZ4H+KdEYioLmGCJyIyKbcJXkTaisgaETkgIvtE5H6dbURE3hCRNBHZIyID/BNu3cYuGiKqSzypwZcBeFgp1R3AEAD3ikgPp20mAOis/ZsOYJ5PowxSX+/MQOrpPKPDIKIQ5TbBK6VOKaV2aK/zARwA0Npps8kAPlQWmwA0EZFEn0cbZB78fDfGv7be6DCIKER51QcvIkkA+gPY7LSqNYATdu8zUPUkABGZLiLbRGRbVlaWl6ESEZE3PE7wItIIwCIADyilnPsd9Hqfq8wpUUq9o5QapJQaFB8f712kRETkFY8SvIhEwJLcP1ZKfaWzSQaAtnbv2wA4WfvwiIiopjyZRSMA/g3ggFLqFRebfQPgFm02zRAAuUqpkHsaNSfREFFd4snNxi4H8H8A9orILm3ZkwDaAYBSaj6ApQAmAkgDUAhgms8jDQIKgNKuduIToIjIaG4TvFJqA9xUTpUlq93rq6CCWYcnlmJ413gsmJZsdChEFOJ4JasPWc+Caw9yhhARGY8J3odWpWYaHQIRkQ0TvA899uUeo0MgIrJhgveTvOJSo0MgohDHBO8nY15ZZ3QIRBTiTJ/gwwyarXgm76IxByYi0pg+wSfFRRsdAhGRIUyf4KMiwo0OgYjIEKZP8EM6xhkdAhGRIUyf4ImIQhUTPBGRSZk+wQ/r0tzoEIiIDGH6BD+iawujQyAiMoTpEzwAdG7RyOgQiIgCLiQS/GfThxh6/I1HspE0Ywmy8nnxExEFTkgk+GbRkQCAds0aGnL89zekAwB2Hs8x5PhEFJpCIsGLCN69ZRAW3jXU6FCIiALGk0f2mcKYHgmGHbusosKwYxNR6AqJGry9lGfHYXK/VgE95sYjZwEAjy/ag5yCkoAem4hCV8gl+Eb162HmxO6GHDunsBT//OGgIccmotATcgne2ZpHhgf0eGLQ7YuJKPSEZIJvFh2JS+Kj8d4tg9CheTT+fesgrHjwSr8dr6SMffBEFHghmeDrhYdh1cPDMVobeB3VPQGdExrj4AvjDY6MiMh3QjLBu1K/Xjj6tW1idBhERD7BBO/E39MpBeyEJ6LAYIJ30qt1LABg6pB2BkdCRFQ7IXOhk6eu6hKPzU+OQkJMFP4wuD0OZ17AXz7daXRYREReY4LXkRATBQDonhiDbi0bY8uxs+jVKhaLdmRgazrvJ0NEwYEJ3g0RwQvX9QYATEm2dNskzVhSi/35JCwiIrfYB09EZFJM8EREJsUEX0Pjehp3d0oiIk8wwdfA/ufG4V+/H4BR3bx/3iu74IkoUDjIWgMNIy1fW1uDnhBFROQJ1uBroXmjSKNDICJyiQm+FqQGcx5P5hb7IRIioqqY4GuhQ/Norz/DB28TUaAwwdfChF4tjQ6BiMgltwleRN4XkUwRSXGxfriI5IrILu3fX30fZt1Uky4aIqJA8WQWzQIAcwF8WM0265VSV/skIiIi8gm3NXil1I8AzgUglhDBWj8RBYav+uCHishuEVkmIj1dbSQi00Vkm4hsy8rK8tGhjdU+zru58GHM70QUIL5I8DsAtFdK9QXwJoDFrjZUSr2jlBqklBoUHx/vg0MbLyYqwugQiIh01TrBK6XylFIXtNdLAUSISPNaRxYkKpTyanvvtiYiqrlaJ3gRaSnadBIRSdb2eba2+w0W5RVM2URUN7mdRSMinwIYDqC5iGQAeAZABAAopeYDuBHA3SJSBqAIwBSlvKzWBrGrusYj9XS+x9uzC56IAsVtgldK3exm/VxYplGGpMfGdcPb6456vD2nzhNRoPBK1loK57QYIqqjmOCJiEyKCT7AhL3wRBQgTPBERCbFBE9EZFJM8EREJsUEH2Cn8/hEJyIKDCZ4IiKTYoI3QE5BCbLyLxodBhGZHBO8DzwxoZtX2/d/fgUufXGln6IhIrJggveBO4d1NDoEIqIqmOB9IIy3KyCiOogJnojIpJjgiYhMigneQLcv2IqJr683OgwiMim394Mn/1mdmml0CERkYqzB1wEHTuUhacYSpGcXGB0KEZkIE3wdsGh7BgBgxf4zBkdCRGbCBE9EZFJM8EREJsUE7yOtmzQwOgQiIgdM8D7ywnW9ar0PBeWDSIiILJjgfWR413ijQyAicsAE7yMiNb8fza/niyz70B7IXVGhUFHB2jwR1Q4TfB2wLOW0w/vfzNuIjk8uNSgaIjILJvg6xNoHv/vEeWMDISJTYIInIjIpJvg6ZpcXtfe0zAs4w4d4E5ELvNlYHSIQXPevnzzefvQr6wAA6bMn+SskIgpirMHXIS8uPWB0CERkIkzwdRinShJRbTDB12EfbEw3OgQiCmJM8D608K6hPt1fWmY+AKCopBzbf8nx6b79ZcfxHLy3/qjRYRARmOB96tKkZj7dX1m5QklZBR5ZuBs3zNuITBczZopLy3163Nq4/q2NeGEJxxKI6gIm+Dps4fYMdHlqGVJO5gIACkv0E/nA51f4NY41BzOxNf2cX49BRL7HaZJBQGljra6GXAtcJH5fmfbBVgCBnY6Z8msuwsME3RNjAnZMIrNhDT4IFJaUAQCUCp1ZNVe/uQETXl9vdBhBb97aI9h4JNvoMMggTPBBwXKXSTOn9693ZmAln0nrcy8tT8Xv393s9ef2ZuRidSp/HsHObYIXkfdFJFNEUlysFxF5Q0TSRGSPiAzwfZihLUy7E7GZK/APfr4bd3y4zegwSHPN3A24fUFw/TxOnCvEp1uOGx1GneJJDX4BgPHVrJ8AoLP2bzqAebUPi+xV3mrexBmeqJamvLMJT3y1F0V+HpMKJm4TvFLqRwDVTaGYDOBDZbEJQBMRSfRVgFT5IBAAyMwrxrHsApfbnjxfhJ+PnNVdtzcjF+nVfNZscgpK8NLyVJRXKCil8Mnm43VqSqm/5ReXGh1CQGVfuGh0CHWOL/rgWwM4Yfc+Q1tWhYhMF5FtIrItKyvLB4cODfZdNMl/W4UR/1zrctsxr6zDze9u0l13zdwNGO702T0Z51FeoXDwdD5yi8yVEGZ9uw/z1h7BygNn8P2+M3jy67345/cHjQ4rIFbuP4Pes37wePuzFy5iW5BPhbW2b2vxcDXT8UWC1/s6dfsSlFLvKKUGKaUGxceb8xmmKc+O8/k+T+ZaLnCqroOmk/YEKG+mTO48noNr5/6EuavTMO61H/G7t3+uTZh1zsXSCgCWe/pcuGiZiXSusMTIkAJmo4tWnCvXz9uIG+fX7uevlEK5kfdPYg9mFb5I8BkA2tq9bwPgpA/2G5Qa1fffpQXVDbKW1eAP67R24nh15SEAQOrp/BrF5WuHzvgvjrJyFdDpphsOZyNpxhJk5BQG7Jg18cvZ2sf36opDuOTJpYZ3g7EGX8kXCf4bALdos2mGAMhVSp3ywX6D1pgeCX7Z77y1abbXb6w6XOP9THh9PXYc9++9bdKzC/DU4r01qtGNffVHn8ZiH8E3u09i9vLUWu+z33M/4M+f7nS73efbLL2XwXIvodr4aLNlBkuB1loKNOsjL0W3UyE0eTJN8lMAPwPoKiIZIvJHEblLRO7SNlkK4CiANADvArjHb9EGif7tmvhlv4t3VTaMXllxyO3289YewTs/Hqmy/MCpPDz/3X7dz+zTbouQU1CCs7UYtLrv0x34aNNx2/58YXXqGXy9M8PtdhuPZGP/yTyHmpz9n/ynm2s3la6opBznC0vx7W7vGqrFpeUB68JQIdhfYW2YsQZfyW1/glLqZjfrFYB7fRaRCfRqFWt0CCgtr8BLWk11ZLcEVDh1S+w8ft7Wt2/vi60n8OzkWPTX7m/zh8HtahWHL2tT1nnZv+nfxuU26dkFtgt7JvRqWetjKqWQkVOEts0a2pblX6wcjE5+cSVuuzwJ9wzv5HZf3Z5ejpsGtsE/bupb67ioKtsgq6FR1C28ktUPruxi/ADyowt3216PfmWdbreHXi3eud73sV1tN6egBN2fXu5ytoV93/bZC5bBTL2uoK92ZLhsHZyv5SCo/SyhZSmntbic4vRif//ecAzD5qzB/pN5AIBZ3+xD8ourbOsz8y9iznLXM3Ock83C7Y4tkMz8YtM82MXd2Mb/dv2KpBlL/DZby1qJET9V4YtLy7H5qHeD10ZjgjeRsvIK22tvZ1F4YtsvOSgqLcf8dVW7fQDgzdWVYwSntNbBm6vTbN0S+07mIiOnEA99sRt3f7QDgOXB4fZecuoff3XFIaw9mIm1BzN1j3n8bKFHg6Y1/Zu33vr4+DnL9QMLavgQFucQ953MxciX1yL5xVW2QW5fctVyUkph+y++mQ6Z8muu7pWjrhLsOz9anhNw4pz3A7rZFy7i3xuOefaz9nrv+n45W4BXVhyyHfPpxSn43Tubqr0Opa7h3SRNZIXdvVwy82vWf17d349en3NOQQkaRIYjKiIc/9v1a5X12RcuYubXezG5X2vc/O4m3Dq0vRZfMZannMZdH23HvD9U3t2iosLx869XM5i872QuJr2xAQCwZ9ZYxERFVFe0KnYcz0GYCPq1bYLyCgUBEBamnx42HjmLu7STkjdcnVieWpyCo1mWRLH2YBYeHtvV6317a9neU9h87BwWbEzH/KkDML6X5XrERdvdj2voufpNy3d/c7JjN54/Zind/9lO/JR2FkM7xqFHK/07jPr6sLcv2IojWQW4aWAbtG3W0DbLLJguIGMN3kTu/3xXrfdRVM0Ut290Enz/51fgun/9VO0+P9t6wlYDPnDK8keSfrYQD2rx3v1xZeL0dHDwvfVHbckdsExHdKXKPhXw3Z6TuP6tjbbYL3lyKW6cv9HlPj78+ReP4rJ34lwh/qcNjNvHUNOB1rUHM3Eqt8hh2ZrUTKxJrdq60esau/vjHbYWyIlzlft52K47rzasNfczeRd1k3x1CfjnI2dx/Vs/obS8Qne9tVvHk++utj00SilsOXYOF8u06yi0wPf+6v2EgVO5RS5bn4HABG8iJWX6fxze+HJ7hm5N3N7KA5mY9MZ6W1PbWrMpLXf9x2d9WMkWu/57vZPJF9vc1ybf33DMq6dGLd5Z9cRkf+fKw9q8+x3Hz9sSzKsrDqH/c55fCapnT4Z+Qnh80R6cK6gca9j7a65D68uV2z7YajuprTuUhXs+3o5pC7Zi2oKtVbbddeJ8zYKuBWtSn/jGerxr99jGCxfLkFtUiv2n8nQ/U1hShscW7caO4+dx6rz+U8usJ4ej2Rd019urSev1X2vS8FOapZKwPOU0fvv2z8jIKdLd1puJA9e8uQG3fVD15xMoTPBUxf2f7XK7zb6TeRg2Z43t/f6TeTheTd+qJwnMU8/pDA7f8/EObDmm37e88sAZrLKv5YrjQOsN8ypr7p1nLgNg6RrKKfS+KX7wdD4ullU9cdnXXr/cnlHlwqI7P9zmUDvNLSy1XX1r71xBCd5am4Zb39+CpXtP25bbd7M4j2voWZri+aUq2Rcu4vOtrqeWntaZjbVkzylbBaDfsz+g77OVJ8sV+8+gy1PLsDcjFx9t+gU9/vq9Q4uiuj56V7+b9hdX3f+Z++sTnP3j+4P4w3uW2VcnnC5Kc255eNNCyL5g7JXTTPB+su7R4UaHEFAT36j+4Rz+GPR19ttqbrWwZE9lQisuLbd1nQBAXrFjIq3JrJY1BzOx6ehZjHvtRzz+5R4Ajt0y//DgHjiXPLkUhSVlSJqxBH2f+wEDntN/FKPerJ2HF+7Gm6sOQynltgUGWKbJPvTFLt2bz9kny5yCEtz90XY8vmivy8Q75O+r8NLyVIcT4u6MXAybswbrD2dVucr69VWHUVJWgWvmbsDyfacd1n2x7QSGzVnjMBB84FQe9p2sWvu3Ssu8gG5PL7e9LyqtfUvW3qEz+UjLrLy6esX+M8gvLjX8il1PcJDVT9rHRRsdArlQXVcSAPylBjXAaXbN8MW7TuK1Kf0d1p/SqeXqySuqPNmU2PVH67UKnL284hC6uXjEod6dFr/a8att+qc9+7KcyS/G1vQcWwzW7qzOCY0dPjNvrf7Mqrl2M6v0OHfJWK/4Tcu8gIHtLQ+xdzXGc+JcIRJiojD21XVO+9TvWnHlVaeLBp1r7NP/u93h/eurDuP1VYfRIzEGS+8f5nK/f/qv8ffTZ4IncvLdntrfaWPky2txy5D2Xn/uj/+p2l9bVl6Bx7RWgTt3unhoyrCX1ugut87ksfez3Vxv++T34pIDWHPQchfYRXcP9Sged22ho04tCL3uD+eB1yNZFxATFYFhc9bgN/1bw7nBlZl/Efd+vANL9p7CFZ2a46M7Buvu829LD+DPIzs7zNSqqPD8GmC9MQV73++r7JY8ca4Qw+aswfu3DUJJWQUGd4hD0+hID49Uc0zwRH5wNKsAs77Vvx1EdZy7IvadzMXz3+3HpqO1m7vuanZUiYtZK1b2Scqa3AHgkYWenXD0+uc92f7xRXvx+KK9SJ89qUrCHfXyOnRq0QgA8PVO/e6oJXstJ+kNadlIzy5AUnNLi3rWN/uQkVOEyf1a4YOf0pFT4NhH/o7d4LAnSsoqkJFTiJgGEbjuXz9hwbRkW2z2rIPe89cdxZZj55DcoRm++JNnJ8naYB+8H3VJqPqDJvLGpDc21Dq5e8PT8QdPL/apbuBdj3ON/u/LDuhOr/RkINnKfprigo3pWHngjO1Gcc69dbOXpXo1n77XrO8x8uV1+HTzcWTkFOHfG44BqDpQbN2ltTWSUYOLvWqCNXgisnmtFncp9Ye313lXo9Yz69v9LltT3t4wzpl1arLzjCfn+fpf77DMcvK2RVNbrMH7EW9bSsGmNrehNouUGlzQZPXpluNImrEEJ50Geq3dW9bB9kDdfogJnojIjrX/3huHnbqMfq/NqXelPEAPnWGC96NGUewBIwoFq3VuF1GdQD1VjAnej565pofRIRBRHRSoK1yZ4P3I27sbEhH5EhO8H/HRYURkJCZ4P+IsGiIyEhO8H8U2YBcNERmHCd6PYhsywRORcZjg/ayr0133iIgChQnezyb1STQ6BCIKUUzwftYwMtzoEIgoRDHBExGZFBM8EZFJMcETEZkUEzwRkUkxwftZRDi/YiIyBrOPn01Jbos/XdXR6DCIKAQxwftZ/XrheGJCd6PDIKIQxARPRGRSTPABMn/qQKNDIKIQwwQfIK2aRBkdAhGFGCb4AElqHm10CEQUYpjgAyQmKgJzbuxjdBhEFEKY4ImITMqjBC8i40XkoIikicgMnfXDRSRXRHZp//7q+1CDHx/CTUSBVM/dBiISDuBfAMYAyACwVUS+UUrtd9p0vVLqaj/EaBrjeiYYHQIRhRBPavDJANKUUkeVUiUAPgMw2b9hmZMIH8JNRIHjSYJvDeCE3fsMbZmzoSKyW0SWiUhPvR2JyHQR2SYi27KysmoQLhERecqTBK9X7VRO73cAaK+U6gvgTQCL9XaklHpHKTVIKTUoPj7eq0CJiMg7niT4DABt7d63AXDSfgOlVJ5S6oL2eimACBFp7rMoiYjIa54k+K0AOotIBxGJBDAFwDf2G4hIS9E6mEUkWdvvWV8HS0REnnM7i0YpVSYi9wH4HkA4gPeVUvtE5C5t/XwANwK4W0TKABQBmKKUcu7GISKiAHKb4AFbt8tSp2Xz7V7PBTDXt6GZ0w8PXomp721GZv5Fo0MhIpPjlawB1iWhMbbMHI302ZOMDoWITI4J3kDdWjY2OgQiMjEmeANN7qd3OQERkW8wwRtodPcWRodARCbGBG+gzgmNMXVIOwBAbIMI3HZZkrEBEZGpMMEbbEKvRADA7Ot7Y8aEbnj+ul4GR0REZsEEb7DLOzXHz0+MxITeiYiKCMe4Ho53nOzfrokxgRFR0GOCrwMSYxvoLk99fjymXNpWdx0RkTtM8HVMTIPKh4JERYRDdO/1RkTknkdXslLgREWEY+FdQ9EyJkp3/bG/T8SO4zm4Yd7PAY6MiIINa/B10KVJzdC2WUPLG6cKvIhgYPtmWPPIcEzo1dLlPhbfe7ntdWJsFJo2tLQMOjSPxjv/N9DnMROR5x4d1zUgx2GCDxIjusbj8+lDbO87NI/GvKmuE3VcdKTt9ZCOcdjx9Bgc+dtErHlkOIZ39Xz+/WWXxNleT+qd6HK7+Mb1ba+tJxOr0d3dP6qwR2KMR/EkxTVE+uxJePeWQejYPNqjzxDVNYPaNw3IcZjg6zhrBb5ZdH0M7hhX7bYjusZjw+Mj8Oi4rmjTtHLgdvYNvSEiCA+z7M2bJwc+PLayptE+riHev22QQ+sAAB4Z2wVbZ45GnzaxAIB2cdH47aA2tvWv/q4vAODavq0QE6XfKzjnxj5Y+dCVbuOJbWg5cY3pkYDVjwx3WOfpScJMeLuL4NQtQL+rTPB1XHKHZgCA3/TXv63B1pmj8cw1PWzv2zRtiHtHdHJ4/mv9euEOn6kXJh5dVHVN31YY6FTTGNktAf3aNrG9T589CfeN7AwAeOYay5Maxe41ADSOikD67El44+b+mH1DnyrHaR/XEJ1aNEKnFpXJKn32JOyZNRbPTe6JRXcPdRnj7wZZZhk9fXUPLL1/GOqF+X5Q+vnJPau0Ssb0cN0qiaxX+Wf11KTuaGftbvMR63eT+vx4LH/A/UnRnn2LDACW/mWYz08Srr6bT+4YbHv98JguPj2mOy/UtetLAnQzdSb4Oq59XDTSZ0/CFZ31H5AV37g+2sd5l0BEBLOu7YnE2Khqk4/3qVJp+wei6+vX1CfqdPOse3QEoiLCqyyPiYrALUOTMLB9M/z3j8m6MTWIDHdYnva3idjw+AiPI35svOu+0HWPDsc3912O/xuahJUPXYXv/nyFbd27twzC9w9ciQ9vT67yuSs6Vf6surZsjEfs+ltjG0RU2X7Fg94lacDy3Vi/M/u43Hnbbvxl9cNXoUerGCy6+zKsdWoN1UazhpXdg9MuT8JdV12CVQ9fhcvsvpc/j+rss+N5wlV3XqATf6vYKPxlVGfENAjM/BYmeBNoHGVJGi1j9WfeuPLzE6Ow9pHhmONUqz74wnj8dlAbPHV1d4fl7mrH1oST0Lgyjohw15/59r4r8Mmdg12utxcXbenjt+/rdyU60vUfj7UIURFhSJ89CfcM7+Ty6uH2cdHo06aJ5fiN6qNX61iH9V1bNsaVXeKRPnuSrRbct20TzJ860NZCGtY5Htf2bYVR3SzjHtaW2PypA2z7aeh0Mlx4l+sWi55erWM9brlYf1cAoGN8IwCWk3GSD8czrPt6fUo/PHNNT8yY0A2XaMf6fPoQ/HlkJwDA/GrGkKyGOVVs6tfzPmV9csfgKjWDlGfHYefTYzB1SHtb5SEQJvROxENjuji0sP2J0yRN4NKkZnh9Sj+M7eE4q+ab+y7HrhPnq/1sWJigc0Ijh2X164Vjzo19be+vH9AaX+341aFW/rff9EanFo6f69kqFi/f1Bdjelqa6Lv/OhZhOn+Pn9wxGC1jo2wJxt4H0y5FSVlFleU9WsVgzg19MM5p5tD1A1pjwcZ0jOhWOXDcNDoS3/35Cny35xTmrzuCm5Pb4r6RnVFUUo5OLRohM6/YodvqD8nt8PTiFAxo1wS3Xd4Bm4+exeMTuul9XWgYGY7CkvIqy1+6oQ9eWp6KBdOSEVkvDLOu7YlZ11Z2U839/QD8er4IbZo2QLeWjTGuZ2U5WjmdmC9NaoY+bWKxJyMX8Y3rI0t7OMybN/fHNX1b6ca1/IErMfqVdbrrJvZuiaV7T+uus7fmkeH4ZPMveHf9MZfbREeGo8Cu/DMndkd84/p44PNdtmXTr+yI3q1jdVudgzvG2caSxtv9LP9zezLyi0tx3yc7HbZvEBGOO4d1sMW0ZeZoPPO/FCzeZXks9L0jLkFSXDQe/XIPbh3aHs0b1cfLKw4BsJzEi0sr0DQ6EolO33Gj+vUAra4wrHM81j82AsPmrAFgqQRU2HWhbHlyFJL/tsr2vlOLRkjLvGB7f+iFCejy1DKX39nj47uhQ/No3PXRdkzq43qigj8wwZuE3q2H+7RpYquBVqd/u6b450190aF5NJo0rNqFYD8jx+r3g9vp7uuGgZWDq7E6+wLg0FR3NqKaGT6/1bmqt0+bJroPT+nVOharDmQCAOIb1UfrJpWDzi2crjEICxPsf24cIsLDEBEehmtdJFHAkgRPni+qsrxv2yb45M4hOp+waBAZbjshTkl2/O70anOL7r4M5RUK4WECpYAdx3MwWBuP0eN8snVc1xiA+wTfoXk0Zk7qYUumS/8yDBPfWO+wTbhOS+G6/q1xXf/WOHg6HxuPZCM8TFx2KbpyVZd4ALAl+Cs6NceGtGz0bdsEuUWlAIAnJnRDbIMIh++rf9umGN0jAYM7xKFVkyiEh4ktwb/2u354a+0RdIyPrjIO5axts4aYOqQdPtp0HBHhYbhoV8mw/31Z/sAwJMY0QN/nfgAAXJrU1GHM5b4RnTB3TZrtfUJMfUzu1wqtmjTAnlljEROl/zfhL0zwBAC40S4xO7M+XTdArUpDNKymW8deQkwUElxchOZLlpNN5fshbmZQVUdg6W8vLfdsZO/avq0Q1ygSPVrF4MdHR+Avn+102RK0T/hdWzZG11oO2H573xVYdygT943sjENn8tEpvhGyLlzEruPncZM2oG7/a2jtlmxnNw518IXx+OVsIbokNMb4XpU15vTZk5A0Y4nbGPS+pd3PjEV5hUIzrbLz1KTueGHJATww2nGw+JFxXbE74zzWH84GAGx+crRtXaCTO8A+ePJCsN42IRie/v7BtEt9ti9rzrXuc0jHOHSMb+SQfHu2cj1N742b+9tmQbWLa+gwLbaD1r9unSlT3RhLTfRuE2ubldUloTHCwgQJMVH44q6htuR6+xUdEFkvDP+5PbnKuAhg6WLskuD9ieY6rRX88R2DMbB9UzSMDLddkBTbIMJ2fAC4Y1hH7H5mLC7XWqOfTR/i8XhSILEGT2710+5o2T3I5pkHU4ujuq4pT311z2VYdzALf7qqI5SyDJ6mPj++ygylLTNHoXH9mtUmP5iWjF0ncrA6NbPW8QKWGUCHM/O9+kyv1rE49MIEnxzf3qCkZrbuvkV3X+Z2e/sZUfYtrJuT22H94WxseXKUz2P0FhM8uXV1n1bo366pQz821T0D2jXFgHaO1y3oTT9t0dj7LqbR3Vtg5YFMNIuOxMhuCYhvFIWPNh336qpoPb1ax+rWwv1l5UNXIb+41K/HmNg7UXdcyAhM8OSRYEzuEeGWHsh6elN56qCYqHoO0xjrkvlTBzoMPPZuE1tnkpg3qhuMNiMmeDKtaZcn4XxhCaZf2dHoUHQtu38YMnIqZ+Ts+utYA6OpXr3wMNQLD44TJVVigifTiooIxxMTu7vf0CDdE2McxjXC/HCbBQptPCUTEZkUEzwRkUkxwRMRmRQTPBGRSTHBExGZFBM8EZFJMcETEZkUEzwRkUmJUsbca09EsgD8UsOPNweQ7cNwggHLHBpY5tBQmzK3V0rFe7KhYQm+NkRkm1JqkNFxBBLLHBpY5tAQqDKzi4aIyKSY4ImITCpYE/w7RgdgAJY5NLDMoSEgZQ7KPngiInIvWGvwRETkBhM8EZFJBV2CF5HxInJQRNJEZIbR8XhDRNqKyBoROSAi+0Tkfm15MxFZISKHtf+b2n3mCa2sB0VknN3ygSKyV1v3hojlEdMiUl9EPteWbxaRpIAXVIeIhIvIThH5Tntv6jKLSBMR+VJEUrWf99AQKPOD2u91ioh8KiJRZiuziLwvIpkikmK3LCBlFJFbtWMcFpFbPQpYKRU0/wCEAzgCoCOASAC7AfQwOi4v4k8EMEB73RjAIQA9AMwBMENbPgPAS9rrHloZ6wPooJU9XFu3BcBQAAJgGYAJ2vJ7AMzXXk8B8LnR5dZieQjAJwC+096buswA/gPgDu11JIAmZi4zgNYAjgFooL3/AsBtZiszgCsBDACQYrfM72UE0AzAUe3/ptrrpm7jNfoPwcsvdyiA7+3ePwHgCaPjqkV5/gdgDICDABK1ZYkADuqVD8D32neQCCDVbvnNAN6230Z7XQ+Wq+XE4HK2AbAKwEhUJnjTlhlADCzJTpyWm7nMrQGc0BJQPQDfARhrxjIDSIJjgvd7Ge230da9DeBmd7EGWxeN9ZfIKkNbFnS0pld/AJsBJCilTgGA9n8LbTNX5W2tvXZe7vAZpVQZgFwAcX4phOdeA/AYgAq7ZWYuc0cAWQA+0Lql3hORaJi4zEqpXwH8E8BxAKcA5CqlfoCJy2wnEGWsUe4LtgSv91TioJvnKSKNACwC8IBSKq+6TXWWqWqWV/cZQ4jI1QAylVLbPf2IzrKgKjMsNa8BAOYppfoDKICl6e5K0JdZ63eeDEtXRCsA0SIytbqP6CwLqjJ7wJdlrFHZgy3BZwBoa/e+DYCTBsVSIyISAUty/1gp9ZW2+IyIJGrrEwFkastdlTdDe+283OEzIlIPQCyAc74viccuB3CtiKQD+AzASBH5COYucwaADKXUZu39l7AkfDOXeTSAY0qpLKVUKYCvAFwGc5fZKhBlrFHuC7YEvxVAZxHpICKRsAxCfGNwTB7TRsr/DeCAUuoVu1XfALCOit8KS9+8dfkUbWS9A4DOALZozcB8ERmi7fMWp89Y93UjgNVK67QzglLqCaVUG6VUEiw/r9VKqakwd5lPAzghIl21RaMA7IeJywxL18wQEWmoxToKwAGYu8xWgSjj9wDGikhTrbU0VltWvUAPUPhggGMiLLNPjgCYaXQ8XsZ+BSzNqj0Admn/JsLSx7YKwGHt/2Z2n5mplfUgtJF2bfkgACnaurmovCo5CsBCAGmwjNR3NLrcdjEPR+Ugq6nLDKAfgG3az3oxLDMfzF7mZwGkavH+F5bZI6YqM4BPYRljKIWlVv3HQJURwO3a8jQA0zyJl7cqICIyqWDroiEiIg8xwRMRmRQTPBGRSTHBExGZFBM8EZFJMcETEZkUEzwRkUn9P3FByBSD+ZspAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[\"costs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ann.predict(X_test_scaled.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7433333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions_categorical = np.argmax(predictions,axis=0)\n",
    "y_test_oh = one_hot(test_y)\n",
    "true_categorical = np.argmax(y_test_oh,axis=0)\n",
    "\n",
    "print(np.sum((predictions_categorical == true_categorical))/y_test_oh.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 600)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
